{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
      "0    1     M14860    M                298.1                    308.6   \n",
      "1    2     L47181    L                298.2                    308.7   \n",
      "2    3     L47182    L                298.1                    308.5   \n",
      "3    4     L47183    L                298.2                    308.6   \n",
      "4    5     L47184    L                298.2                    308.7   \n",
      "\n",
      "   Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Target Failure Type  \n",
      "0                    1551         42.8                0       0   No Failure  \n",
      "1                    1408         46.3                3       0   No Failure  \n",
      "2                    1498         49.4                5       0   No Failure  \n",
      "3                    1433         39.5                7       0   No Failure  \n",
      "4                    1408         40.0                9       0   No Failure  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   UDI                      10000 non-null  int64  \n",
      " 1   Product ID               10000 non-null  object \n",
      " 2   Type                     10000 non-null  object \n",
      " 3   Air temperature [K]      10000 non-null  float64\n",
      " 4   Process temperature [K]  10000 non-null  float64\n",
      " 5   Rotational speed [rpm]   10000 non-null  int64  \n",
      " 6   Torque [Nm]              10000 non-null  float64\n",
      " 7   Tool wear [min]          10000 non-null  int64  \n",
      " 8   Target                   10000 non-null  int64  \n",
      " 9   Failure Type             10000 non-null  object \n",
      "dtypes: float64(3), int64(4), object(3)\n",
      "memory usage: 781.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "df.head(2)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
      "14   15     L47194    L                298.6                    309.2   \n",
      "50   51     L47230    L                298.9                    309.1   \n",
      "62   63     L47242    L                298.8                    309.0   \n",
      "69   70     L47249    L                298.9                    309.0   \n",
      "70   71     M14930    M                298.9                    309.0   \n",
      "\n",
      "    Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Target  \\\n",
      "14                    2035         19.6               40       0   \n",
      "50                    2861          4.6              143       1   \n",
      "62                    1829         22.9              172       0   \n",
      "69                    1410         65.7              191       1   \n",
      "70                    1924         22.6              193       0   \n",
      "\n",
      "     Failure Type  DBSCAN_Cluster  \n",
      "14     No Failure              -1  \n",
      "50  Power Failure              -1  \n",
      "62     No Failure              -1  \n",
      "69  Power Failure              -1  \n",
      "70     No Failure              -1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "\n",
    "# Select columns to be considered for anomaly detection\n",
    "cols = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']\n",
    "\n",
    "# Standardize the data\n",
    "X = StandardScaler().fit_transform(data[cols])\n",
    "\n",
    "# Apply DBSCAN for anomaly detection\n",
    "# eps: Maximum distance between two samples for them to be considered as in the same neighborhood\n",
    "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "data['DBSCAN_Cluster'] = dbscan.fit_predict(X)\n",
    "\n",
    "# Identify outliers (points labeled as -1)\n",
    "outliers = data[data['DBSCAN_Cluster'] == -1]\n",
    "\n",
    "# Save the outliers to a new CSV file for further analysis if needed\n",
    "outliers.to_csv(r'../dataset/outliers_dbscan.csv', index=False)\n",
    "\n",
    "# Display the first few outliers\n",
    "print(outliers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers detected by Z-Score method: 178\n",
      "Number of outliers detected by IQR method: 459\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "\n",
    "# Columns to check for anomalies\n",
    "cols = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']\n",
    "\n",
    "# Z-Score Method\n",
    "z = np.abs(stats.zscore(data[cols]))\n",
    "z_threshold = 3\n",
    "outliers_z = np.where(z > z_threshold)\n",
    "\n",
    "# IQR Method\n",
    "Q1 = data[cols].quantile(0.25)\n",
    "Q3 = data[cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = ((data[cols] < (Q1 - 1.5 * IQR)) | (data[cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# Print the number of outliers detected by each method\n",
    "print(f\"Number of outliers detected by Z-Score method: {len(np.unique(outliers_z[0]))}\")\n",
    "print(f\"Number of outliers detected by IQR method: {outliers_iqr.sum()}\")\n",
    "\n",
    "# Optional: Save the outliers to separate CSV files for further analysis\n",
    "data.iloc[outliers_z[0]].to_csv(r'../dataset/z_score_outliers.csv', index=False)\n",
    "data[outliers_iqr].to_csv(r'../dataset/iqr_outliers.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UDI Product ID Type  Air temperature [K]  Process temperature [K]  \\\n",
      "14      15     L47194    L                298.6                    309.2   \n",
      "50      51     L47230    L                298.9                    309.1   \n",
      "69      70     L47249    L                298.9                    309.0   \n",
      "70      71     M14930    M                298.9                    309.0   \n",
      "101    102     L47281    L                298.8                    308.8   \n",
      "...    ...        ...  ...                  ...                      ...   \n",
      "9951  9952     L57131    L                298.2                    307.8   \n",
      "9970  9971     H39384    H                298.4                    308.1   \n",
      "9974  9975     L57154    L                298.6                    308.2   \n",
      "9988  9989     L57168    L                298.9                    308.6   \n",
      "9991  9992     M24851    M                298.9                    308.4   \n",
      "\n",
      "      Rotational speed [rpm]  Torque [Nm]  Tool wear [min]  Target  \\\n",
      "14                      2035         19.6               40       0   \n",
      "50                      2861          4.6              143       1   \n",
      "69                      1410         65.7              191       1   \n",
      "70                      1924         22.6              193       0   \n",
      "101                     1991         20.7               59       0   \n",
      "...                      ...          ...              ...     ...   \n",
      "9951                    1355         59.4              115       0   \n",
      "9970                    1891         24.7              158       0   \n",
      "9974                    1361         68.2              172       1   \n",
      "9988                    1771         24.1              213       0   \n",
      "9991                    1827         26.1                5       0   \n",
      "\n",
      "       Failure Type  anomaly  \n",
      "14       No Failure       -1  \n",
      "50    Power Failure       -1  \n",
      "69    Power Failure       -1  \n",
      "70       No Failure       -1  \n",
      "101      No Failure       -1  \n",
      "...             ...      ...  \n",
      "9951     No Failure       -1  \n",
      "9970     No Failure       -1  \n",
      "9974  Power Failure       -1  \n",
      "9988     No Failure       -1  \n",
      "9991     No Failure       -1  \n",
      "\n",
      "[2187 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'cols' contains the columns to be used for anomaly detection\n",
    "cols = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']\n",
    "X = data[cols]\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "iso_forest.fit(X_scaled)\n",
    "\n",
    "# Predict anomalies (-1 for outliers and 1 for inliers)\n",
    "data['anomaly'] = iso_forest.predict(X_scaled)\n",
    "\n",
    "# Filter out the anomalies\n",
    "anomalies = data[data['anomaly'] == -1]\n",
    "\n",
    "# Display or process the anomalies\n",
    "print(anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']].values\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the Autoencoder architecture\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 14  # You might need to tune this\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
    "decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "# Compile the Autoencoder\n",
    "autoencoder.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the Autoencoder\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=32, shuffle=True, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Use the Autoencoder for anomaly detection\n",
    "# Compute the reconstruction loss on the training set\n",
    "reconstructed = autoencoder.predict(X_scaled)\n",
    "mse = np.mean(np.power(X_scaled - reconstructed, 2), axis=1)\n",
    "\n",
    "# Determine a threshold for anomaly detection\n",
    "threshold = np.quantile(mse, 0.95)  # Adjust based on your dataset\n",
    "\n",
    "# Detect anomalies\n",
    "outliers = mse > threshold\n",
    "print(\"Number of anomalies detected:\", np.sum(outliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']].values\n",
    "\n",
    "# Initialize the Nearest Neighbors model\n",
    "knn = NearestNeighbors(n_neighbors=5)  # Consider adjusting n_neighbors based on your dataset\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(X)\n",
    "\n",
    "# Compute the distances and indices of the K-nearest neighbors to each point\n",
    "distances, indices = knn.kneighbors(X)\n",
    "\n",
    "# Compute the average distance to the K-nearest neighbors\n",
    "avg_distance = np.mean(distances, axis=1)\n",
    "\n",
    "# Determine a threshold for anomaly detection (e.g., based on a percentile)\n",
    "threshold = np.percentile(avg_distance, 95)  # Adjust the percentile value as needed\n",
    "\n",
    "# Flag points with an average distance above the threshold as anomalies\n",
    "anomalies = avg_distance > threshold\n",
    "\n",
    "# Print the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Adjust n_components based on your analysis\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Inverse transform to reconstruct the original data\n",
    "X_inverse = pca.inverse_transform(X_pca)\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error = np.sum(np.square(X_scaled - X_inverse), axis=1)\n",
    "\n",
    "# Determine a threshold for anomaly detection\n",
    "threshold = np.percentile(reconstruction_error, 95)  # Adjust the percentile as needed\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = reconstruction_error > threshold\n",
    "\n",
    "# Print the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5001\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Lahiye\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load and prepare the dataset\n",
    "df = pd.read_csv(r'../dataset/predictive_maintenance.csv')\n",
    "X = df[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]\n",
    "y = df['Target']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model (for demonstration, we re-train it here, but typically you'd load a pre-trained model)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to a file (optional, for loading later)\n",
    "pickle.dump(model, open('rf_model.pkl', 'wb'))\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        json_data = request.json\n",
    "        input_data = pd.DataFrame([json_data])\n",
    "        prediction = model.predict(input_data)\n",
    "        return jsonify(prediction=int(prediction[0]))\n",
    "    except Exception as e:\n",
    "        return jsonify(error=str(e)), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=5001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
